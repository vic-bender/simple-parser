import re
from typing import List, Tuple, Any, Union
from ASTNodeDefs import *

# Code written by Victor A. Bender

# A type alias for clarity, representing parts of expressions
# that can be a full ASTNode or a simple token tuple (like for numbers/identifiers).
ExprType = Union[ASTNode, Tuple[str, Any]]

class Lexer:
    """
    The Lexer (also known as a tokenizer or scanner) is responsible for breaking
    the raw source code string into a stream of meaningful tokens.
    """
    def __init__(self, code: str) -> None:
        """
        Initializes the Lexer.

        Args:
            code: The source code string to be tokenized.
        """
        self.code = code
        self.tokens: List[Tuple[str, Any]] = []
        # A list of tuples where each tuple contains a token name and a regex pattern.
        # The order is important, as it determines matching priority.
        self.token_specs = [
            ('NUMBER',     r'\d+'),
            ('IDENTIFIER', r'[A-Za-z_][A-Za-z0-9_]*'),
            ('IF',         r'if'),
            ('ELSE',       r'else'),
            ('FOR',        r'for'),
            ('TO',         r'to'),
            ('PRINT',      r'print'),
            ('AND',        r'and'),
            ('OR',         r'or'),
            ('NOT',        r'not'),
            ('PLUS',       r'\+'),
            ('MINUS',      r'-'),
            ('MULTIPLY',   r'\*'),
            ('DIVIDE',     r'/'),
            ('MODULO',     r'%'),
            ('EQ',         r'=='),
            ('NEQ',        r'!='),
            ('GREATER',    r'>'),
            ('LESS',       r'<'),
            ('EQUALS',     r'='),
            ('LPAREN',     r'\('),
            ('RPAREN',     r'\)'),
            ('COMMA',      r','),
            ('COLON',      r':'),
            ('SKIP',       r'[ \t\n]+'),  # Skips whitespace and newlines
            ('MISMATCH',   r'.'),         # Catches any other character
        ]
        # A single, combined regex for efficient matching.
        self.token_regex = '|'.join(f'(?P<{name}>{regex})' for name, regex in self.token_specs)

    # TODO: Implement this function
    def tokenize(self) -> List[Tuple[str, Any]]:
       tokens: List[Tuple[str, Any]] = [] # setup for later
       keywords = {}

       for name, regex in self.token_specs: # finds and maps keywords w/ identifiers
            if regex.isalpha() == True and regex == regex.lower():
                keywords[regex] = name
       
       for match in re.finditer(self.token_regex, self.code): # loops over all regex matches in order
            match_kind = match.lastgroup
            lexeme = match.group()

            if match_kind == 'SKIP': # case handling for all required cases
                continue
            elif match_kind == 'MISMATCH':
                raise SyntaxError("Mismatch Error")
            elif match_kind == 'NUMBER':
                tokens.append((match_kind, int(lexeme)))
            elif match_kind == 'HEXNUMBER':
                tokens.append((match_kind, lexeme))
            elif match_kind == 'BOOL':
                tokens.append((match_kind, lexeme == "True"))
            elif match_kind == 'IDENTIFIER':
                mapped_kind = keywords.get(lexeme)
                if mapped_kind is not None:
                    tokens.append((mapped_kind, lexeme))
                else:
                    tokens.append((match_kind, lexeme))
            else:
                tokens.append((match_kind, lexeme))
                
       tokens.append(('EOF', None)) # End of File handling
       self.tokens = tokens
       return tokens


class Parser:
    """
    The Parser takes the list of tokens generated by the Lexer and builds an
    Abstract Syntax Tree (AST). The AST is a tree representation of the source
    code's structure that is much easier to work with for later stages like
    interpretation or compilation.
    """
    def __init__(self, tokens: List[Tuple[str, Any]]) -> None:
        """
        Initializes the Parser.

        Args:
            tokens: A list of tokens from the Lexer.
        """
        self.tokens = tokens
        self.pos = 0  # The parser's current position in the token stream

    def current_token(self) -> Tuple[str, Any]:
        """A helper function to look at the current token without consuming it."""
        return self.tokens[self.pos]

    def advance(self) -> None:
        """A helper function to consume the current token and move to the next one."""
        self.pos += 1

    def expect(self, kind: str) -> Tuple[str, Any]:
        """
        Checks if the current token matches an expected type. If so, it consumes
        the token. If not, it raises a syntax error. This is crucial for
        enforcing the language's grammar rules.
        """
        if self.current_token()[0] == kind:
            token = self.current_token()
            self.advance()
            return token
        else:
            raise SyntaxError(f"Expected {kind} but got {self.current_token()[0]} at position {self.pos}")

    def parse(self) -> List[ASTNode]:
        """
        Why this function is needed: This is the main entry point for the parsing process.
        It orchestrates the entire parsing operation by repeatedly parsing the fundamental
        unit of our language: a statement.

        What this function does: It creates an empty list to hold the statements of the
        program. It then loops as long as it has not reached the 'EOF' token. In each
        iteration, it calls `parse_statement()` to parse a single statement and appends
        the resulting AST node to the list. Finally, it returns the list of statement
        nodes, which represents the complete program.
        """
        statements = []
        while self.current_token()[0] != 'EOF':
            statements.append(self.parse_statement())
        return statements

    # TODO: Implement this function
    def parse_statement(self) -> ASTNode:
        current_type = self.current_token()[0] # setup

        if current_type == 'IDENTIFIER': # identifier handling
            variable_token = self.current_token()
            self.advance()
            self.expect('EQUALS')
            
            assigned_value_expression = self.parse_boolean_expression()
            return Assignment(variable_token, assigned_value_expression)

        elif current_type == 'IF': # special case handling (defined below)
            return self.parse_if_stmt()
        elif current_type == 'FOR':
            return self.parse_for_stmt()
        elif current_type == 'PRINT':
            return self.parse_print_stmt()
        
        else: # edge case handling
            raise SyntaxError("Unexpected Token")

    # TODO: Implement this function
    def parse_if_stmt(self) -> IfStatement:
        self.expect('IF') # if statement handling
        condition = self.parse_boolean_expression()
        self.expect('COLON')
        then_block = self.parse_block()

        else_block = None
        if self.current_token()[0] == 'ELSE':  # optional else handling
            self.advance()
            self.expect('COLON')
            else_block = self.parse_block()

        return IfStatement(condition, then_block, else_block)

    # TODO: Implement this function
    def parse_for_stmt(self) -> ForStatement: # work here is self-explanatory
        self.expect('FOR')
        var = self.expect('IDENTIFIER')
        self.expect('EQUALS')
        start = self.parse_expression()
        self.expect('TO')
        end = self.parse_expression()
        self.expect('COLON')
        body = self.parse_block()
        
        return ForStatement(var, start, end, body)

    # TODO: Implement this function
    def parse_print_stmt(self) -> PrintStatement: # work here is self-explanatory
        self.expect('PRINT')
        self.expect('LPAREN')
        
        if self.current_token()[0] == 'RPAREN': # handling and calling of arg list function
            contents = []
        else:
            contents = self.parse_arg_list()

        self.expect('RPAREN')
        
        return PrintStatement(contents)

    # TODO: Implement this function
    def parse_block(self) -> Block:
        statements = [] # setup
        done = False

        while not done: # main loop for execution
            token_type = self.current_token()[0]
            
            if token_type == 'ELSE' or token_type == 'EOF':
                done = True
            else:
                stmt = self.parse_statement()
                statements.append(stmt)

        block_node = Block(statements) # final return
        return block_node

    # TODO: Implement this function
    def parse_arg_list(self) -> List[ExprType]:
        arguments = [] # setup
        expr = self.parse_expression()
        arguments.append(expr)

        token_type = self.current_token()[0] # advance and parse
        while token_type == 'COMMA':
            self.advance()
            expr = self.parse_expression()
            
            arguments.append(expr)
            token_type = self.current_token()[0]

        return arguments

    # TODO: Implement this function
    def parse_boolean_expression(self) -> ExprType:
        left = self.parse_boolean_term() # setup
        token_type = self.current_token()[0]
        
        while token_type == 'OR': # separate into left and right, processing left first
            self.advance()
            right = self.parse_boolean_term()
            left = LogicalOperation(left, ('OR', 'or'), right)
            token_type = self.current_token()[0]
        
        return left

    # TODO: Implement this function
    def parse_boolean_term(self) -> ExprType:
        left = self.parse_boolean_factor() # setup
        token_type = self.current_token()[0]
        
        while token_type == 'AND': # same type of logic as above (processing left first)
            self.advance()
            right = self.parse_boolean_factor()
            left = LogicalOperation(left, ('AND', 'and'), right)
            token_type = self.current_token()[0]
        
        return left

    # TODO: Implement this function
    def parse_boolean_factor(self) -> ExprType: # self-explanatory
        token_type = self.current_token()[0]
        
        if token_type == 'NOT':
            self.advance()
            expr = self.parse_boolean_factor()
            return UnaryOperation(('NOT', 'not'), expr)
        
        return self.parse_comparison()

    # TODO: Implement this function
    def parse_comparison(self) -> ExprType: # self-explanatory, same type of structure as above
        left = self.parse_expression()
        token_type = self.current_token()[0]
        
        while token_type in ('EQ', 'NEQ', 'LESS', 'GREATER'): # recursive to handle x < y < z like cases
            op = self.current_token()
            self.advance()
            right = self.parse_expression()
            left = BinaryOperation(left, op, right)
            token_type = self.current_token()[0]
        
        return left

    # TODO: Implement this function
    def parse_expression(self) -> ExprType:
        left = self.parse_term()
        token_type = self.current_token()[0]
        
        while token_type in ('PLUS', 'MINUS'): # left-associative recursion as above
            op = self.current_token()
            self.advance()
            right = self.parse_term()
            left = BinaryOperation(left, op, right)
            token_type = self.current_token()[0]
        
        return left

    # TODO: Implement this function
    def parse_term(self) -> ExprType: # same structure as parse_expression
        left = self.parse_factor()
        token_type = self.current_token()[0]
        
        while token_type in ('MULTIPLY', 'DIVIDE', 'MODULO'):
            op = self.current_token()
            self.advance()
            right = self.parse_factor()
            left = BinaryOperation(left, op, right)
            token_type = self.current_token()[0]
        
        return left

    # TODO: Implement this function
    def parse_factor(self) -> ExprType:
        token_type = self.current_token()[0]
        
        if token_type in ('PLUS', 'MINUS'): # handles stuff like +x or -x
            op = self.current_token()
            self.advance()
            expr = self.parse_factor()
            return UnaryOperation(op, expr)
        
        if token_type == 'LPAREN': # handles parentheses
            self.advance()
            expr = self.parse_boolean_expression()
            self.expect('RPAREN')
            return expr
        
        if token_type in ('NUMBER', 'IDENTIFIER'): # handles simple numbers or var names
            current_token = self.current_token()
            self.advance()
            return current_token
        
        raise SyntaxError("Unexpected Token") # edge case

    # TODO: Implement this function
    def parse_primary(self) -> ExprType:
        token_type = self.current_token()[0]
        
        if token_type == 'NUMBER': # handles numbers
            tok = self.current_token()
            self.advance()
            return tok
        
        if token_type == 'IDENTIFIER': # handles var names
            tok = self.current_token()
            self.advance()
            return tok
        
        if token_type == 'LPAREN': # parenthesis handling
            self.advance()
            expr = self.parse_boolean_expression()
            self.expect('RPAREN')
            return expr
        
        raise SyntaxError("Unexpected Token") # edge case